[INFO 20:16:06] pymarl Running command 'my_main'
[INFO 20:16:06] pymarl Started run with ID "1"
[WARNING 20:16:06] my_main CUDA flag use_cuda was switched OFF automatically because no CUDA devices are available!
[INFO 20:16:06] my_main Runner specified in config: parallel
[INFO 20:16:06] my_main Runner resolved in args: parallel
[INFO 20:16:06] my_main Experiment Parameters:
[INFO 20:16:06] my_main 

{   'action_selector': 'soft_policies',
    'add_value_last_step': True,
    'agent': 'rnn',
    'agent_output_type': 'pi_logits',
    'batch_size': 10,
    'batch_size_run': 10,
    'buffer_cpu_only': True,
    'buffer_size': 10,
    'checkpoint_path': '',
    'common_reward': True,
    'critic_baseline_fn': 'coma',
    'critic_q_fn': 'coma',
    'critic_train_mode': 'seq',
    'critic_train_reps': 1,
    'critic_type': 'coma_critic',
    'entropy_coef': 0.001,
    'env': 'simcity',
    'env_args': {   'grid_x': 4,
                    'grid_y': 4,
                    'key': 'simcity',
                    'map_name': 'simcity',
                    'seed': 524729644,
                    'time_limit': 100},
    'evaluate': False,
    'gamma': 0.99,
    'grad_norm_clip': 10,
    'hidden_dim': 64,
    'hypergroup': None,
    'label': 'default_label',
    'learner': 'coma_learner',
    'learner_log_interval': 100,
    'load_step': 0,
    'local_results_path': 'results',
    'log_interval': 100,
    'lr': 0.0003,
    'mac': 'basic_mac',
    'mask_before_softmax': True,
    'name': 'coma',
    'obs_agent_id': True,
    'obs_individual_obs': False,
    'obs_last_action': False,
    'optim_alpha': 0.99,
    'optim_eps': 1e-05,
    'q_nstep': 10,
    'render': False,
    'repeat_id': 1,
    'reward_scalarisation': 'sum',
    'runner': 'parallel',
    'runner_log_interval': 100,
    'save_model': False,
    'save_model_interval': 10,
    'save_replay': False,
    'seed': 524729644,
    'standardise_returns': False,
    'standardise_rewards': True,
    't_max': 20050000,
    'target_update_interval_or_tau': 200,
    'test_greedy': True,
    'test_interval': 100,
    'test_nepisode': 20,
    'use_cuda': False,
    'use_rnn': True,
    'use_tensorboard': True,
    'use_wandb': False,
    'wandb_mode': 'offline',
    'wandb_project': None,
    'wandb_save_model': False,
    'wandb_team': None}

[INFO 20:16:06] my_main *******************
[INFO 20:16:06] my_main Tensorboard logging dir:
[INFO 20:16:06] my_main /home/jin/Documents/GitHub/epymarl/results/tb_logs/coma_seed524729644_simcity_2024-12-10 20:16:06.519885
[INFO 20:16:06] my_main *******************
[INFO 20:16:06] my_main Final parsed config: {'runner': 'parallel', 'mac': 'basic_mac', 'env': 'simcity', 'common_reward': True, 'reward_scalarisation': 'sum', 'env_args': {'key': 'simcity', 'map_name': 'simcity', 'seed': 524729644, 'time_limit': 100, 'grid_x': 4, 'grid_y': 4}, 'batch_size_run': 10, 'test_nepisode': 20, 'test_interval': 100, 'test_greedy': True, 'log_interval': 100, 'runner_log_interval': 100, 'learner_log_interval': 100, 't_max': 20050000, 'use_cuda': False, 'buffer_cpu_only': True, 'use_tensorboard': True, 'use_wandb': False, 'wandb_team': None, 'wandb_project': None, 'wandb_mode': 'offline', 'wandb_save_model': False, 'save_model': False, 'save_model_interval': 10, 'checkpoint_path': '', 'evaluate': False, 'render': False, 'load_step': 0, 'save_replay': False, 'local_results_path': 'results', 'gamma': 0.99, 'batch_size': 10, 'buffer_size': 10, 'lr': 0.0003, 'optim_alpha': 0.99, 'optim_eps': 1e-05, 'grad_norm_clip': 10, 'add_value_last_step': True, 'agent': 'rnn', 'hidden_dim': 64, 'obs_agent_id': True, 'obs_last_action': False, 'repeat_id': 1, 'label': 'default_label', 'hypergroup': None, 'action_selector': 'soft_policies', 'mask_before_softmax': True, 'target_update_interval_or_tau': 200, 'obs_individual_obs': False, 'agent_output_type': 'pi_logits', 'learner': 'coma_learner', 'critic_q_fn': 'coma', 'standardise_returns': False, 'standardise_rewards': True, 'use_rnn': True, 'critic_baseline_fn': 'coma', 'critic_train_mode': 'seq', 'critic_train_reps': 1, 'entropy_coef': 0.001, 'q_nstep': 10, 'critic_type': 'coma_critic', 'name': 'coma', 'seed': 524729644}
[INFO 20:16:06] my_main run.py Using runner: parallel
[INFO 20:16:06] my_main Initialized runner: ParallelRunner
[INFO 20:16:06] my_main Beginning training for 20050000 timesteps
Process Process-2:
Process Process-3:
Process Process-5:
Process Process-1:
Process Process-4:
Process Process-6:
Process Process-7:
Process Process-8:
Process Process-9:
Process Process-10:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jin/Documents/GitHub/epymarl/src/runners/parallel_runner.py", line 296, in env_worker
    _, reward, terminated, truncated, env_info = env.step(actions)
Traceback (most recent call last):
  File "/home/jin/Documents/GitHub/epymarl/src/runners/parallel_runner.py", line 296, in env_worker
    _, reward, terminated, truncated, env_info = env.step(actions)
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jin/Documents/GitHub/epymarl/src/runners/parallel_runner.py", line 296, in env_worker
    _, reward, terminated, truncated, env_info = env.step(actions)
  File "/home/jin/Documents/GitHub/epymarl/src/envs/simcity_wrapper.py", line 139, in step
    actions_list = actions.squeeze(0).cpu().numpy().tolist()
  File "/home/jin/Documents/GitHub/epymarl/src/envs/simcity_wrapper.py", line 139, in step
    actions_list = actions.squeeze(0).cpu().numpy().tolist()
  File "/home/jin/Documents/GitHub/epymarl/src/runners/parallel_runner.py", line 296, in env_worker
    _, reward, terminated, truncated, env_info = env.step(actions)
  File "/home/jin/Documents/GitHub/epymarl/src/envs/simcity_wrapper.py", line 139, in step
    actions_list = actions.squeeze(0).cpu().numpy().tolist()
  File "/home/jin/Documents/GitHub/epymarl/src/envs/simcity_wrapper.py", line 139, in step
    actions_list = actions.squeeze(0).cpu().numpy().tolist()
Traceback (most recent call last):
ValueError: cannot select an axis to squeeze out which has size not equal to one
ValueError: cannot select an axis to squeeze out which has size not equal to one
ValueError: cannot select an axis to squeeze out which has size not equal to one
ValueError: cannot select an axis to squeeze out which has size not equal to one
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jin/Documents/GitHub/epymarl/src/runners/parallel_runner.py", line 296, in env_worker
    _, reward, terminated, truncated, env_info = env.step(actions)
  File "/home/jin/Documents/GitHub/epymarl/src/envs/simcity_wrapper.py", line 139, in step
    actions_list = actions.squeeze(0).cpu().numpy().tolist()
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jin/Documents/GitHub/epymarl/src/runners/parallel_runner.py", line 296, in env_worker
    _, reward, terminated, truncated, env_info = env.step(actions)
  File "/home/jin/Documents/GitHub/epymarl/src/envs/simcity_wrapper.py", line 139, in step
    actions_list = actions.squeeze(0).cpu().numpy().tolist()
ValueError: cannot select an axis to squeeze out which has size not equal to one
Traceback (most recent call last):
Traceback (most recent call last):
ValueError: cannot select an axis to squeeze out which has size not equal to one
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jin/Documents/GitHub/epymarl/src/runners/parallel_runner.py", line 296, in env_worker
    _, reward, terminated, truncated, env_info = env.step(actions)
  File "/home/jin/Documents/GitHub/epymarl/src/runners/parallel_runner.py", line 296, in env_worker
    _, reward, terminated, truncated, env_info = env.step(actions)
  File "/home/jin/Documents/GitHub/epymarl/src/envs/simcity_wrapper.py", line 139, in step
    actions_list = actions.squeeze(0).cpu().numpy().tolist()
  File "/home/jin/Documents/GitHub/epymarl/src/envs/simcity_wrapper.py", line 139, in step
    actions_list = actions.squeeze(0).cpu().numpy().tolist()
Traceback (most recent call last):
Traceback (most recent call last):
ValueError: cannot select an axis to squeeze out which has size not equal to one
ValueError: cannot select an axis to squeeze out which has size not equal to one
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jin/anaconda3/envs/epymarl/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jin/Documents/GitHub/epymarl/src/runners/parallel_runner.py", line 296, in env_worker
    _, reward, terminated, truncated, env_info = env.step(actions)
  File "/home/jin/Documents/GitHub/epymarl/src/runners/parallel_runner.py", line 296, in env_worker
    _, reward, terminated, truncated, env_info = env.step(actions)
  File "/home/jin/Documents/GitHub/epymarl/src/envs/simcity_wrapper.py", line 139, in step
    actions_list = actions.squeeze(0).cpu().numpy().tolist()
  File "/home/jin/Documents/GitHub/epymarl/src/envs/simcity_wrapper.py", line 139, in step
    actions_list = actions.squeeze(0).cpu().numpy().tolist()
ValueError: cannot select an axis to squeeze out which has size not equal to one
ValueError: cannot select an axis to squeeze out which has size not equal to one
