[DEBUG 18:25:26] git.cmd Popen(['git', 'version'], cwd=/home/jin/Documents/GitHub/epymarl, stdin=None, shell=False, universal_newlines=False)
[DEBUG 18:25:26] git.cmd Popen(['git', 'version'], cwd=/home/jin/Documents/GitHub/epymarl, stdin=None, shell=False, universal_newlines=False)
[DEBUG 18:25:26] git.cmd Popen(['git', 'diff', '--cached', '--abbrev=40', '--full-index', '--raw'], cwd=/home/jin/Documents/GitHub/epymarl, stdin=None, shell=False, universal_newlines=False)
[DEBUG 18:25:26] git.cmd Popen(['git', 'diff', '--abbrev=40', '--full-index', '--raw'], cwd=/home/jin/Documents/GitHub/epymarl, stdin=None, shell=False, universal_newlines=False)
[DEBUG 18:25:26] git.cmd Popen(['git', 'cat-file', '--batch-check'], cwd=/home/jin/Documents/GitHub/epymarl, stdin=<valid stream>, shell=False, universal_newlines=False)
[DEBUG 18:25:26] git.cmd Popen(['git', 'diff', '--cached', '--abbrev=40', '--full-index', '--raw'], cwd=/home/jin/Documents/GitHub/epymarl, stdin=None, shell=False, universal_newlines=False)
[DEBUG 18:25:26] git.cmd Popen(['git', 'diff', '--abbrev=40', '--full-index', '--raw'], cwd=/home/jin/Documents/GitHub/epymarl, stdin=None, shell=False, universal_newlines=False)
[DEBUG 18:25:26] git.cmd Popen(['git', 'cat-file', '--batch-check'], cwd=/home/jin/Documents/GitHub/epymarl, stdin=<valid stream>, shell=False, universal_newlines=False)
[DEBUG 18:25:26] git.cmd Popen(['git', 'diff', '--cached', '--abbrev=40', '--full-index', '--raw'], cwd=/home/jin/Documents/GitHub/epymarl, stdin=None, shell=False, universal_newlines=False)
[DEBUG 18:25:26] git.cmd Popen(['git', 'diff', '--abbrev=40', '--full-index', '--raw'], cwd=/home/jin/Documents/GitHub/epymarl, stdin=None, shell=False, universal_newlines=False)
[DEBUG 18:25:26] git.cmd Popen(['git', 'cat-file', '--batch-check'], cwd=/home/jin/Documents/GitHub/epymarl, stdin=<valid stream>, shell=False, universal_newlines=False)
[INFO 18:25:26] root Saving to FileStorageObserver in results/sacred.
[DEBUG 18:25:26] pymarl Using capture mode "fd"
[INFO 18:25:26] pymarl Running command 'my_main'
[INFO 18:25:26] pymarl Started run with ID "43"
[DEBUG 18:25:26] pymarl Starting Heartbeat
[DEBUG 18:25:26] my_main Started
[INFO 18:25:26] my_main Experiment Parameters:
[INFO 18:25:26] my_main 

{   'action_selector': 'epsilon_greedy',
    'add_value_last_step': True,
    'agent': 'rnn',
    'agent_output_type': 'q',
    'batch_size': 32,
    'batch_size_run': 1,
    'buffer_cpu_only': True,
    'buffer_size': 5000,
    'checkpoint_path': '',
    'common_reward': True,
    'double_q': True,
    'env': 'simcity',
    'env_args': {   'grid_size': 4,
                    'key': 'simcity',
                    'map_name': 'simcity',
                    'seed': 566595340,
                    'time_limit': 100},
    'epsilon_anneal_time': 50000,
    'epsilon_finish': 0.05,
    'epsilon_start': 1.0,
    'evaluate': False,
    'evaluation_epsilon': 0.0,
    'gamma': 0.99,
    'grad_norm_clip': 10,
    'hidden_dim': 64,
    'hypergroup': None,
    'hypernet_embed': 64,
    'hypernet_layers': 2,
    'label': 'default_label',
    'learner': 'q_learner',
    'learner_log_interval': 2,
    'load_step': 0,
    'local_results_path': 'results',
    'log_interval': 2,
    'lr': 0.0005,
    'mac': 'basic_mac',
    'mixer': 'qmix',
    'mixing_embed_dim': 32,
    'name': 'qmix',
    'obs_agent_id': True,
    'obs_individual_obs': False,
    'obs_last_action': False,
    'optim_alpha': 0.99,
    'optim_eps': 1e-05,
    'render': False,
    'repeat_id': 1,
    'reward_scalarisation': 'sum',
    'runner': 'episode',
    'runner_log_interval': 2,
    'save_model': False,
    'save_model_interval': 10,
    'save_replay': False,
    'seed': 566595340,
    'standardise_returns': False,
    'standardise_rewards': True,
    't_max': 3,
    'target_update_interval_or_tau': 200,
    'test_greedy': True,
    'test_interval': 2,
    'test_nepisode': 20,
    'use_cuda': True,
    'use_rnn': False,
    'use_tensorboard': False,
    'use_wandb': False,
    'wandb_mode': 'offline',
    'wandb_project': None,
    'wandb_save_model': False,
    'wandb_team': None}

[DEBUG 18:25:26] root simcity_wrapper Initializing SimCityWrapper
[DEBUG 18:25:26] root simcity_wrapper Players initialized: P1, P2, P3
[DEBUG 18:25:26] root simcity_wrapper Number of agents: 3
[DEBUG 18:25:26] root simcity_wrapper Episode limit set to: 100
[DEBUG 18:25:26] root simcity_wrapper Observation size per agent: 66
[DEBUG 18:25:26] root simcity_wrapper State size: 198
[DEBUG 18:25:26] root simcity_wrapper Number of actions per agent: 48
[DEBUG 18:25:26] root simcity_wrapper Resetting environment during initialization.
[DEBUG 18:25:26] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:26] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:26] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:26] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:26] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:26] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:26] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:26] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:26] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:26] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:26] root State shape after reset: (1, 198)
[DEBUG 18:25:26] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:26] root simcity_wrapper Initial observation shape is correct.
[DEBUG 18:25:26] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:26] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:26] root simcity_wrapper Initial state shape is correct.
[DEBUG 18:25:26] root simcity_wrapper Environment info: {'state_shape': 198, 'obs_shape': 66, 'n_actions': 48, 'n_agents': 3, 'episode_limit': 100}
[INFO 18:25:27] my_main Beginning training for 3 timesteps
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[13, 20, 29]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([13, 20, 29], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 13
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 20
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 29
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[13, 20, 29]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[ 5, 25,  8]], device='cuda:0')
[DEBUG 18:25:27] root return_mean: 1.0, return_std: 0.0
[DEBUG 18:25:27] root ep_length_mean: 1.0
[INFO 18:25:27] my_main t_env: 1 / 3
[INFO 18:25:27] my_main Estimated time left: 0 seconds. Time passed: 0 seconds
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root test_return_mean: 1.0, test_return_std: 0.0
[DEBUG 18:25:27] root test_ep_length_mean: 1.0
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[43, 41, 26]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([43, 41, 26], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 43
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 41
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 26
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[43, 41, 26]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[10, 12, 38]], device='cuda:0')
[INFO 18:25:27] my_main Recent Stats | t_env:          2 | Episode:        2
ep_length_mean:            1.0000	epsilon:                   1.0000	return_mean:               1.0000	return_std:                0.0000
test_ep_length_mean:       1.0000	test_return_mean:          1.0000	test_return_std:           0.0000	
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[18, 37, 20]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([18, 37, 20], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 18
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 37
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 20
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[18, 37, 20]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[ 0, 13, 21]], device='cuda:0')
[DEBUG 18:25:27] root return_mean: 1.0, return_std: 0.0
[DEBUG 18:25:27] root ep_length_mean: 1.0
[INFO 18:25:27] my_main t_env: 3 / 3
[INFO 18:25:27] my_main Estimated time left: -. Time passed: 0 seconds
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([21, 21, 21], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 21
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 21
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 21
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[21, 21, 21]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 21, 21]], device='cuda:0')
[DEBUG 18:25:27] root test_return_mean: 1.0, test_return_std: 0.0
[DEBUG 18:25:27] root test_ep_length_mean: 1.0
[DEBUG 18:25:27] root simcity_wrapper Resetting the underlying SimCityEnv.
[DEBUG 18:25:27] root simcity_wrapper Number of agents after reset: 3
[DEBUG 18:25:27] root simcity_wrapper Updated observation size per agent: 66
[DEBUG 18:25:27] root simcity_wrapper Updated state size: 198
[DEBUG 18:25:27] root simcity_wrapper Updated number of actions per agent: 48
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Observation shape after reset: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Reset observation shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root State shape after reset: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Reset state shape is correct.
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Pre-transition data at time step 0: {'state': [array([[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20., -1.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20.,
        30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
        15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 15.,
        20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30., 20., 20.,
        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
        -1., -1., -1.]], dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.],
        [15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         15., 20., 30., 15., 20., 30., 15., 20., 30., 15., 20., 30.,
         20., 20., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
         -1., -1., -1., -1., -1., -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions: tensor([[ 0, 19, 29]], device='cuda:0')
[DEBUG 18:25:27] root Executing step with actions: tensor([ 0, 19, 29], device='cuda:0')
[DEBUG 18:25:27] root simcity_wrapper Agent P1 taking action 0
[DEBUG 18:25:27] root environment.py Player P2 resources after action: {'money': 19, 'reputation': 17}
[DEBUG 18:25:27] root environment.py Player P2 scores: 0, 0
[INFO 18:25:27] root Turn 0, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P1 received reward: 0
[DEBUG 18:25:27] root simcity_wrapper Agent P2 taking action 19
[INFO 18:25:27] root Turn 1, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P2 received reward: 4
[DEBUG 18:25:27] root simcity_wrapper Agent P3 taking action 29
[INFO 18:25:27] root Turn 2, Environment Score 21.666666666666664
[DEBUG 18:25:27] root simcity_wrapper Agent P3 received reward: -3
[DEBUG 18:25:27] root simcity_wrapper Aggregated total reward (common_reward=True): 1
[DEBUG 18:25:27] root simcity_wrapper Termination status - Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root simcity_wrapper Number of active agents after step: 3
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Step result - Reward: 1, Terminated: [False, False, False], Truncated: [False, False, False]
[DEBUG 18:25:27] root Episode return so far: 1
[DEBUG 18:25:27] root Post-transition data at time step 0: {'actions': tensor([[ 0, 19, 29]], device='cuda:0'), 'terminated': [(True,)], 'reward': [(1,)]}
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root simcity_wrapper Generated global state with shape: (1, 198)
[DEBUG 18:25:27] root simcity_wrapper Available actions shape: (1, 3, 48)
[DEBUG 18:25:27] root simcity_wrapper Generated aggregated observations with shape: (1, 3, 66)
[DEBUG 18:25:27] root Last data before episode ends: {'state': [array([[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  18.,  20.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
         45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
         20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
         15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,
         30.,  15.,  20.,  30.,  19.,  23.,   1.,  -1.,  -1.,  -1.,  -1.,
         -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.]],
      dtype=float32)], 'avail_actions': [array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],
      dtype=float32)], 'obs': [array([[[ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  18.,  20.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.],
        [ 45., -10.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  25.,  10.,  30.,  15.,  20.,  30.,  15.,  20.,
          30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,
          15.,  20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  15.,
          20.,  30.,  15.,  20.,  30.,  15.,  20.,  30.,  19.,  23.,
           1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,
          -1.,  -1.,  -1.,  -1.,  -1.,  -1.]]], dtype=float32)]}
[DEBUG 18:25:27] root Selected actions for last state: tensor([[21, 28, 16]], device='cuda:0')
[INFO 18:25:27] my_main Recent Stats | t_env:          4 | Episode:        4
ep_length_mean:            1.0000	epsilon:                   1.0000	return_mean:               1.0000	return_std:                0.0000
test_ep_length_mean:       1.0000	test_return_mean:          1.0000	test_return_std:           0.0000	
[DEBUG 18:25:27] root simcity_wrapper Closing environment.
[INFO 18:25:27] my_main Finished Training
[DEBUG 18:25:28] my_main Finished after 0:00:02.
[INFO 18:25:28] pymarl Completed after 0:00:02
[DEBUG 18:25:28] pymarl Stopping Heartbeat
